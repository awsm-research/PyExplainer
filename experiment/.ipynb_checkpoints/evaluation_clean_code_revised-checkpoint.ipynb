{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, time, re, sys, operator\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.metrics import auc, classification_report, roc_auc_score, f1_score, matthews_corrcoef, balanced_accuracy_score, r2_score , confusion_matrix, precision_score, recall_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from my_util import *\n",
    "from lime.lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "# from pyexplainer.pyexplainer_pyexplainer import PyExplainer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "from pyexplainer.pyexplainer_pyexplainer import PyExplainer\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './dataset/'\n",
    "result_dir = './eval_result/'\n",
    "dump_dataframe_dir = './dump_df/'\n",
    "pyExp_dir = './pyExplainer_obj/'\n",
    "other_object_dir = './other_object/'\n",
    "# proj_name = 'qt' # ['openstack','qt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_lime(proj_name):\n",
    "#     global_model, correctly_predict_df, indep, dep, feature_df = prepare_data_for_testing(proj_name)\n",
    "#     all_eval_result = pd.DataFrame()\n",
    "    \n",
    "#     for i in range(0,len(feature_df)):\n",
    "#         X_explain = feature_df.iloc[[i]]\n",
    "\n",
    "#         row_index = str(X_explain.index[0])\n",
    "\n",
    "#         py_exp = pickle.load(open(pyExp_dir+proj_name+'_rulefit_crossoverinterpolation_'+row_index+'.pkl','rb'))\n",
    "#         lime_exp = pickle.load(open(pyExp_dir+proj_name+'_lime_'+row_index+'.pkl','rb'))\n",
    "\n",
    "#         # this data can be used for both local and global model\n",
    "#         py_exp_synthetic_data = py_exp['synthetic_data'].values\n",
    "#         # this data can be used with global model only\n",
    "#         lime_exp_synthetic_data = lime_exp['synthetic_instance_for_global_model']\n",
    "#         # this data can be used with local model only\n",
    "#         lime_exp_synthetic_data_local = lime_exp['synthetic_instance_for_lobal_model']\n",
    "        \n",
    "#         display(X_explain)\n",
    "#         display(lime_exp_synthetic_data[:5,:])\n",
    "#         display(lime_exp_synthetic_data_local[:5,:])\n",
    "        \n",
    "#         break\n",
    "        \n",
    "# test_lime('openstack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "flip_sign_dict = {\n",
    "    '<': '>=',\n",
    "    '>': '<=',\n",
    "    '=': '!=',\n",
    "    '>=': '<',\n",
    "    '<=': '>',\n",
    "    '!=': '=='\n",
    "}\n",
    "\n",
    "'''\n",
    "    input: rule (str)\n",
    "'''\n",
    "def flip_rule(rule):\n",
    "    rule = re.sub(r'\\b=\\b',' = ',rule) # for LIME\n",
    "#     rule = rule.replace('&','and') # for RuleFit\n",
    "    found_rule = re.findall('.* <=? [a-zA-Z]+ <=? .*', rule) # for LIME\n",
    "    ret = ''\n",
    "    \n",
    "    # for LIME that has condition like this: 0.53 < nref <= 0.83\n",
    "    if len(found_rule) > 0:\n",
    "        found_rule = found_rule[0]\n",
    "    \n",
    "        var_in_rule = re.findall('[a-zA-Z]+',found_rule)\n",
    "\n",
    "        var_in_rule = var_in_rule[0]\n",
    "        \n",
    "        splitted_rule = found_rule.split(var_in_rule)\n",
    "        splitted_rule[0] = splitted_rule[0] + var_in_rule # for left side\n",
    "        splitted_rule[1] = var_in_rule + splitted_rule[1] # for right side\n",
    "        combined_rule = splitted_rule[0] + ' or ' + splitted_rule[1]\n",
    "        ret = flip_rule(combined_rule)\n",
    "        \n",
    "    else:\n",
    "        for tok in rule.split():\n",
    "            if tok in flip_sign_dict:\n",
    "                ret = ret + flip_sign_dict[tok] + ' '\n",
    "            else:\n",
    "                ret = ret + tok + ' '\n",
    "    return ret\n",
    "\n",
    "def get_top_k_global_features(global_model, indep, top_k_global_feature_num = 5):\n",
    "    global_feature_df = pd.DataFrame()\n",
    "    global_feature_df['feature'] = indep\n",
    "    global_feature_df['importance'] = global_model.feature_importances_\n",
    "\n",
    "    global_feature_df = global_feature_df.sort_values(by='importance',ascending=False)\n",
    "\n",
    "    top_k_global_features = list(global_feature_df['feature'])[:top_k_global_feature_num]\n",
    "\n",
    "    return top_k_global_features\n",
    "    \n",
    "def sort_global_feature(global_model, indep):\n",
    "    global_feature_df = pd.DataFrame()\n",
    "    global_feature_df['feature'] = indep\n",
    "    global_feature_df['importance'] = global_model.feature_importances_\n",
    "\n",
    "    global_feature_df = global_feature_df.sort_values(by='importance',ascending=False)\n",
    "\n",
    "    sorted_global_features = list(global_feature_df['feature'])\n",
    "\n",
    "    return sorted_global_features\n",
    "\n",
    "def get_rule_str_of_rulefit(local_rulefit_model):\n",
    "    rule_df = local_rulefit_model.get_rules()\n",
    "#     print(rule_df)\n",
    "    top_k = 5\n",
    "    top_k_positive_rules = rule_df[(rule_df.coef > 0) & (rule_df.type=='rule')].sort_values(\"importance\", ascending=False).head(top_k)\n",
    "#     top_k_positive_rules = rule_df[(rule_df.coef > 0) & (rule_df.type=='rule')].sort_values(\"coef\", ascending=False).head(top_k)\n",
    "\n",
    "    the_best_defective_rule_str = list(top_k_positive_rules['rule'])[0]\n",
    "    \n",
    "    return the_best_defective_rule_str\n",
    "\n",
    "def get_rule_str_of_rulefit_new_version(local_rulefit_model):\n",
    "    rule_df = local_rulefit_model.get_rules()\n",
    "    rule_df =  rule_df[(rule_df.coef > 0) & (rule_df.type=='rule')].sort_values(\"importance\", ascending=False)\n",
    "    \n",
    "    rule_list = list(rule_df['rule'])\n",
    "    dup_feature_in_rule = [] # true or false...\n",
    "    \n",
    "    for r in rule_list:\n",
    "        var_in_rule = re.findall('[a-zA-Z]+', r)\n",
    "        var_count = Counter(var_in_rule)\n",
    "        max_count = max(list(var_count.values()))\n",
    "        \n",
    "        if max_count > 1:\n",
    "            dup_feature_in_rule.append(True)\n",
    "        else:\n",
    "            dup_feature_in_rule.append(False)\n",
    "           \n",
    "    if False not in set(dup_feature_in_rule):\n",
    "#         print('wtf')\n",
    "        rule_df = rule_df.head(5)\n",
    "        the_best_defective_rule_str = list(rule_df['rule'])[0]\n",
    "        \n",
    "    else:\n",
    "        rule_df['contain_dup_var'] = dup_feature_in_rule    \n",
    "        the_best_defective_rule_str = rule_df[rule_df['contain_dup_var']==False].iloc[0]['rule']\n",
    "    \n",
    "    return the_best_defective_rule_str\n",
    "\n",
    "def aggregate_list(l):\n",
    "    return np.mean(l), np.median(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_new_rule_from_rulefit(proj_name):\n",
    "#     global_model, correctly_predict_df, indep, dep, feature_df = prepare_data_for_testing(proj_name)\n",
    "#     all_eval_result = pd.DataFrame()\n",
    "    \n",
    "#     c = 0\n",
    "    \n",
    "# #     py_exp_all_rules = []\n",
    "# #     lime_all_rules = []\n",
    "#     py_exp_all_vars = []\n",
    "#     lime_all_vars = []\n",
    "    \n",
    "#     print('global feature feature importance ranking:')\n",
    "#     print(sort_global_feature(global_model, indep))\n",
    "#     for i in range(0,len(feature_df)):\n",
    "#         X_explain = feature_df.iloc[[i]]\n",
    "\n",
    "#         row_index = str(X_explain.index[0])\n",
    "\n",
    "#         py_exp = pickle.load(open(pyExp_dir+proj_name+'_rulefit_crossoverinterpolation_'+row_index+'_20_rules.pkl','rb'))\n",
    "#         py_exp_local_model = py_exp['local_model']\n",
    "        \n",
    "#         lime_exp = pickle.load(open(pyExp_dir+proj_name+'_lime_'+row_index+'.pkl','rb'))\n",
    "        \n",
    "# #         py_exp_rule = get_rule_str_of_rulefit(py_exp_local_model)\n",
    "#         py_exp_rule_new = get_rule_str_of_rulefit_new_version(py_exp_local_model)\n",
    "#         lime_the_best_defective_rule_str = lime_exp['rule'].as_list()[0][0]\n",
    "\n",
    "#         py_exp_pred = eval_rule(py_exp_rule_new, X_explain)[0]\n",
    "#         lime_pred = eval_rule(lime_the_best_defective_rule_str, X_explain)[0]\n",
    "\n",
    "#         if py_exp_pred == 1:\n",
    "#             py_exp_var_in_rule = list(set(re.findall('[a-zA-Z]+', py_exp_rule_new)))\n",
    "#             py_exp_all_vars.extend(py_exp_var_in_rule)\n",
    "#         if lime_pred == 1:\n",
    "#             lime_var_in_rule = list(set(re.findall('[a-zA-Z]+', lime_the_best_defective_rule_str)))\n",
    "#             lime_all_vars.extend(lime_var_in_rule)\n",
    "            \n",
    "# #         py_exp_all_rules.append(py_exp_rule_new)\n",
    "# #         lime_all_rules.append(lime_the_best_defective_rule_str)\n",
    "        \n",
    "# #         eval_result = eval_rule(lime_the_best_defective_rule_str, X_explain)\n",
    "\n",
    "# #         if eval_result[0]:\n",
    "# #             c =c+1\n",
    "\n",
    "    \n",
    "# #     print(len(set(py_exp_all_rules)))\n",
    "# #     print(len(set(lime_all_rules)))\n",
    "    \n",
    "#     print('pyExplainer var count')\n",
    "#     print(Counter(py_exp_all_vars))\n",
    "#     print('-'*100)\n",
    "#     print('LIME var count')\n",
    "#     print(Counter(lime_all_vars))\n",
    "    \n",
    "# print('openstack')\n",
    "# test_new_rule_from_rulefit('openstack')\n",
    "# print('*'*100)\n",
    "# print('qt')\n",
    "# test_new_rule_from_rulefit('qt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def is_in_top_k_global_features(top_k_global_features, the_best_defective_rule_str):\n",
    "    # remove numeric value\n",
    "    new_the_best_defective_rule_str = re.sub('\\d+','', the_best_defective_rule_str)\n",
    "\n",
    "    # remove special characters\n",
    "    new_the_best_defective_rule_str = re.sub('\\W+',' ',new_the_best_defective_rule_str)\n",
    "    splitted_rule = new_the_best_defective_rule_str.split()\n",
    "\n",
    "    local_feature_count = 0\n",
    "    \n",
    "    found_features = set(splitted_rule).intersection(top_k_global_features)\n",
    "    return list(found_features)\n",
    "\n",
    "# def eval_rule(rule, X_explain):\n",
    "#     var_in_rule = re.findall('[a-zA-Z]+',rule)\n",
    "#     rule = rule.replace('&','and') # just for rulefit\n",
    "#     rule = re.sub(r'\\b=\\b','==',rule)\n",
    "# #             rule = rule.replace('=','==')\n",
    "\n",
    "#     var_dict = {}\n",
    "\n",
    "#     for var in var_in_rule:\n",
    "#         var_dict[var] = float(X_explain[var])\n",
    "\n",
    "#     eval_result = eval(rule,var_dict)\n",
    "#     return eval_result\n",
    "\n",
    "        \n",
    "def prepare_data_for_testing(proj_name, global_model_name = 'RF'):\n",
    "    global_model_name = global_model_name.upper()\n",
    "    global_model = pickle.load(open(proj_name+'_'+global_model_name+'_global_model.pkl','rb'))\n",
    "\n",
    "    correctly_predict_df = pd.read_csv(dump_dataframe_dir+proj_name+'_'+global_model_name+'_correctly_predict_as_defective.csv')\n",
    "    correctly_predict_df = correctly_predict_df.set_index('commit_id')\n",
    "\n",
    "    dep = 'defect'\n",
    "    indep = correctly_predict_df.columns[:-3] # exclude the last 3 columns\n",
    "\n",
    "    feature_df = correctly_predict_df.loc[:, indep]\n",
    "    \n",
    "    return global_model, correctly_predict_df, indep, dep, feature_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_result_df(proj_name, global_model_name):\n",
    "    global_model_name = global_model_name.upper()\n",
    "    if global_model_name not in ['RF','LR']:\n",
    "        print('wrong global model name. the global model name must be RF or LR')\n",
    "        return\n",
    "    \n",
    "    prediction_df_dir = dump_dataframe_dir+proj_name+'_'+global_model_name+'_prediction_result.csv'\n",
    "    correctly_predict_df_dir = dump_dataframe_dir+proj_name+'_'+global_model_name+'_correctly_predict_as_defective.csv'\n",
    "    \n",
    "    if not os.path.exists(prediction_df_dir) or not os.path.exists(correctly_predict_df_dir):\n",
    "        global_model = pickle.load(open(proj_name+'_'+global_model_name+'_global_model.pkl','rb'))\n",
    "\n",
    "        pred = global_model.predict(x_test)\n",
    "        defective_prob = global_model.predict_proba(x_test)[:,1]\n",
    "\n",
    "        prediction_df = x_test.copy()\n",
    "        prediction_df['pred'] = pred\n",
    "        prediction_df['defective_prob'] = defective_prob\n",
    "        prediction_df['defect'] = y_test\n",
    "\n",
    "    #     print('AUC is',roc_auc_score(y_test, defective_prob))\n",
    "        correctly_predict_df = prediction_df[(prediction_df['pred']==1) & (prediction_df['defect']==1)]\n",
    "\n",
    "        print('total correct prediction: {}'.format(str(len(correctly_predict_df))))\n",
    "\n",
    "        prediction_df.to_csv(prediction_df_dir)\n",
    "        correctly_predict_df.to_csv(correctly_predict_df_dir)\n",
    "    \n",
    "    else:\n",
    "        prediction_df = pd.read_csv(prediction_df_dir)\n",
    "        correctly_predict_df = pd.read_csv(correctly_predict_df_dir)\n",
    "        \n",
    "        prediction_df = prediction_df.set_index('commit_id')\n",
    "        correctly_predict_df = correctly_predict_df.set_index('commit_id')\n",
    "        print('total correct prediction: {}'.format(str(len(correctly_predict_df))))\n",
    "        \n",
    "    return prediction_df, correctly_predict_df\n",
    "\n",
    "def get_recall_at_k_percent_effort(percent_effort, result_df_arg, real_buggy_commits):\n",
    "    cum_LOC_k_percent = (percent_effort/100)*result_df_arg.iloc[-1]['cum_LOC']\n",
    "    buggy_line_k_percent =  result_df_arg[result_df_arg['cum_LOC'] <= cum_LOC_k_percent]\n",
    "    buggy_commit = buggy_line_k_percent[buggy_line_k_percent['defect']==True]\n",
    "    recall_k_percent_effort = len(buggy_commit)/float(len(real_buggy_commits))\n",
    "    \n",
    "    return recall_k_percent_effort\n",
    "\n",
    "def eval_global_model(proj_name, prediction_df):\n",
    "    ## since ld metric in openstack is removed by using autospearman, so this code is needed\n",
    "    ## but this is not problem for qt\n",
    "    \n",
    "    if proj_name == 'openstack':\n",
    "        x_train_original, x_test_original = prepare_data_all_metrics(proj_name, mode='all')\n",
    "        prediction_df = prediction_df.copy()\n",
    "#         print('add ld')\n",
    "#         display(x_test_original['ld'])\n",
    "        prediction_df['ld'] = list(x_test_original['ld'])\n",
    "        \n",
    "    prediction_df = prediction_df[['la','ld', 'pred', 'defective_prob' ,'defect']]\n",
    "    prediction_df['LOC'] = prediction_df['la']+prediction_df['ld']\n",
    "    \n",
    "    \n",
    "#     result_df['defect_density'] = result_df['defective_commit_prob']/result_df['LOC']\n",
    "    prediction_df['defect_density'] = prediction_df['defective_prob']/prediction_df['LOC']\n",
    "    prediction_df['actual_defect_density'] = prediction_df['defect']/prediction_df['LOC'] #defect density\n",
    "    \n",
    "    prediction_df = prediction_df.fillna(0)\n",
    "    prediction_df = prediction_df.replace(np.inf, 0)\n",
    "    \n",
    "    prediction_df = prediction_df.sort_values(by='defect_density',ascending=False)\n",
    "#     display(prediction_df.head())\n",
    "#     display(np.sum(prediction_df[prediction_df['la']==0]['defect']))\n",
    "    \n",
    "    actual_result_df = prediction_df.sort_values(by='actual_defect_density',ascending=False)\n",
    "    actual_worst_result_df = prediction_df.sort_values(by='actual_defect_density',ascending=True)\n",
    "\n",
    "    prediction_df['cum_LOC'] = prediction_df['LOC'].cumsum()\n",
    "    actual_result_df['cum_LOC'] = actual_result_df['LOC'].cumsum()\n",
    "    actual_worst_result_df['cum_LOC'] = actual_worst_result_df['LOC'].cumsum()\n",
    "\n",
    "    real_buggy_commits = prediction_df[prediction_df['defect'] == True]\n",
    "    \n",
    "#     display(prediction_df)\n",
    "#     display(real_buggy_commits)\n",
    "    \n",
    "    \n",
    "    AUC = roc_auc_score(prediction_df['defect'], prediction_df['defective_prob'])\n",
    "    f1 = f1_score(prediction_df['defect'], prediction_df['pred'])\n",
    "    \n",
    "    ifa = real_buggy_commits.iloc[0]['cum_LOC']\n",
    "#     print('ifa:',ifa)\n",
    "\n",
    "    cum_LOC_20_percent = 0.2*prediction_df.iloc[-1]['cum_LOC']\n",
    "    buggy_line_20_percent = prediction_df[prediction_df['cum_LOC'] <= cum_LOC_20_percent]\n",
    "    buggy_commit = buggy_line_20_percent[buggy_line_20_percent['defect']==True]\n",
    "    recall_20_percent_effort = len(buggy_commit)/float(len(real_buggy_commits))\n",
    "    \n",
    "    # find P_opt\n",
    "    percent_effort_list = []\n",
    "    predicted_recall_at_percent_effort_list = []\n",
    "    actual_recall_at_percent_effort_list = []\n",
    "    actual_worst_recall_at_percent_effort_list = []\n",
    "    \n",
    "    for percent_effort in np.arange(10,101,10):\n",
    "        predicted_recall_k_percent_effort = get_recall_at_k_percent_effort(percent_effort, prediction_df, real_buggy_commits)\n",
    "        actual_recall_k_percent_effort = get_recall_at_k_percent_effort(percent_effort, actual_result_df, real_buggy_commits)\n",
    "        actual_worst_recall_k_percent_effort = get_recall_at_k_percent_effort(percent_effort, actual_worst_result_df, real_buggy_commits)\n",
    "        \n",
    "        percent_effort_list.append(percent_effort/100)\n",
    "        \n",
    "        predicted_recall_at_percent_effort_list.append(predicted_recall_k_percent_effort)\n",
    "        actual_recall_at_percent_effort_list.append(actual_recall_k_percent_effort)\n",
    "        actual_worst_recall_at_percent_effort_list.append(actual_worst_recall_k_percent_effort)\n",
    "\n",
    "    p_opt = 1 - ((auc(percent_effort_list, actual_recall_at_percent_effort_list) - \n",
    "                 auc(percent_effort_list, predicted_recall_at_percent_effort_list)) /\n",
    "                (auc(percent_effort_list, actual_recall_at_percent_effort_list) -\n",
    "                auc(percent_effort_list, actual_worst_recall_at_percent_effort_list)))\n",
    "\n",
    "    print('AUC: {}, F1: {}, IFA: {}, Recall@20%Effort: {}, Popt: {}'.format(AUC,f1,ifa,recall_20_percent_effort,p_opt))\n",
    "    print(classification_report(prediction_df['defect'], prediction_df['pred']))\n",
    "#     display(cum_LOC_20_percent)\n",
    "#     display(buggy_line_20_percent)\n",
    "# #     display(cum_LOC_20_percent)\n",
    "#     display(buggy_commit)\n",
    "#     print(len(real_buggy_commits))\n",
    "#     display(recall_20_percent_effort)\n",
    "\n",
    "def get_global_model_evaluation_result(proj_name):\n",
    "    print('RF global model result')\n",
    "    rf_prediction_df, rf_correctly_predict_df = get_prediction_result_df(proj_name, 'rf')\n",
    "    eval_global_model(proj_name, rf_prediction_df)\n",
    "\n",
    "    print('-'*100)\n",
    "    \n",
    "    print('LR global model result')\n",
    "    lr_prediction_df, lr_correctly_predict_df = get_prediction_result_df(proj_name, 'lr')\n",
    "    eval_global_model(proj_name, lr_prediction_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ3 evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def eval_rule(rule, x_df):\n",
    "    var_in_rule = list(set(re.findall('[a-zA-Z]+', rule)))\n",
    "    \n",
    "    rule = re.sub(r'\\b=\\b','==',rule)\n",
    "    if 'or' in var_in_rule:\n",
    "        var_in_rule.remove('or')\n",
    "        \n",
    "    rule = rule.replace('&','and')\n",
    "    \n",
    "    eval_result_list = []\n",
    "    \n",
    "#     print(rule)\n",
    "\n",
    "    for i in range(0,len(x_df)):\n",
    "        x = x_df.iloc[[i]]\n",
    "        col = x.columns\n",
    "        var_dict = {}\n",
    "\n",
    "        for var in var_in_rule:\n",
    "            var_dict[var] = float(x[var])\n",
    "\n",
    "#         print(var_dict)\n",
    "        \n",
    "        # if the rule does not satisfy clean commit, the truth value of the inversed rule when applied to clean commit is true\n",
    "        eval_result = eval(rule,var_dict)\n",
    "        eval_result_list.append(eval_result)\n",
    "        \n",
    "#         print(eval_result)\n",
    "#         break\n",
    "        \n",
    "    return eval_result_list\n",
    "\n",
    "# def summarize_rule_eval_result(py_exp_rule_str, lime_rule_str, x_df, ground_truth):\n",
    "# #     print('Rulefit')\n",
    "#     py_exp_all_eval_result = eval_rule(py_exp_rule_str, x_df)\n",
    "# #     print('LIME')\n",
    "#     lime_all_eval_result = eval_rule(lime_rule_str, x_df)\n",
    "\n",
    "# #     print(py_exp_rule_str)\n",
    "# #     print(lime_rule_str)\n",
    "    \n",
    "# #     tmp_df = x_df.copy()\n",
    "# #     tmp_df['ground_truth'] = ground_truth\n",
    "# #     tmp_df_clean = tmp_df[tmp_df['ground_truth']==False]\n",
    "    \n",
    "# #     display(tmp_df_clean)\n",
    "    \n",
    "#     py_exp_result_df = pd.DataFrame()\n",
    "#     py_exp_result_df['ground_truth'] = ground_truth\n",
    "#     py_exp_result_df['rule_result'] = py_exp_all_eval_result\n",
    "#     py_exp_result_df = py_exp_result_df[py_exp_result_df['rule_result']==True] # get commit that matches counter rule\n",
    "    \n",
    "# #     print('py_exp_result_df')\n",
    "# #     display(py_exp_result_df[py_exp_result_df['ground_truth']==False])\n",
    "# #     print(len(py_exp_result_df))\n",
    "#     # find ratio of clean commit\n",
    "#     py_exp_satisfy_rule_ratio = 100*(len(py_exp_result_df[py_exp_result_df['ground_truth']==False])/len(py_exp_result_df)) if len(py_exp_result_df) > 0 else 0\n",
    "    \n",
    "#     lime_result_df = pd.DataFrame()\n",
    "#     lime_result_df['ground_truth'] = ground_truth\n",
    "#     lime_result_df['rule_result'] = lime_all_eval_result\n",
    "    \n",
    "#     lime_result_df = lime_result_df[lime_result_df['rule_result']==True] # get commit that matches counter rule\n",
    "    \n",
    "# #     print(len(lime_result_df))\n",
    "    \n",
    "#     # find ratio of clean commit\n",
    "# #     print('lime_result_df')\n",
    "# #     display(lime_result_df[lime_result_df['ground_truth']==False])\n",
    "#     lime_satisfy_rule_ratio = 100*(len(lime_result_df[lime_result_df['ground_truth']==False])/len(lime_result_df))  if len(lime_result_df) > 0 else 0\n",
    "    \n",
    "# #     print(len(py_exp_result_df[py_exp_result_df['ground_truth']==False]))\n",
    "# #     print(len(lime_result_df[lime_result_df['ground_truth']==False]))\n",
    "    \n",
    "#     return py_exp_satisfy_rule_ratio, lime_satisfy_rule_ratio\n",
    "\n",
    "def summarize_rule_eval_result(rule_str, x_df):\n",
    "#     print('Rulefit')\n",
    "    all_eval_result = eval_rule(rule_str, x_df)\n",
    "    all_eval_result = np.array(all_eval_result).astype(bool)\n",
    "    \n",
    "#     result_df = pd.DataFrame()\n",
    "#     result_df['ground_truth'] = ground_truth\n",
    "#     result_df['rule_result'] = all_eval_result\n",
    "#     result_df = result_df[result_df['rule_result']==True] # get commit that matches counter rule\n",
    "    \n",
    "#     print('py_exp_result_df')\n",
    "#     display(py_exp_result_df[py_exp_result_df['ground_truth']==False])\n",
    "#     print(len(py_exp_result_df))\n",
    "    # find ratio of clean commit\n",
    "#     satisfy_rule_ratio = 100*(len(result_df[result_df['ground_truth']==False])/len(result_df)) if len(result_df) > 0 else 0\n",
    "\n",
    "    return all_eval_result\n",
    "\n",
    "'''\n",
    "    input:\n",
    "        local_model: local model of RuleFit\n",
    "        X_explain: an instance to be explained\n",
    "        \n",
    "    return:\n",
    "        g2_guide, g4_guide (string)\n",
    "        more info of g2/g4 guidance refers to SQAPlanner paper\n",
    "'''\n",
    "\n",
    "def get_g2_g4_guidance(local_model, X_explain):\n",
    "    rules = local_model.get_rules()\n",
    "    rules = rules[(rules['type']=='rule') & (rules['coef'] < 0) & (rules['importance'] > 0)]\n",
    "    rules_list = list(rules['rule'])\n",
    "    \n",
    "    rule_eval_result = []\n",
    "\n",
    "    for r in rules_list:\n",
    "        py_exp_pred = eval_rule(r, X_explain)[0]\n",
    "        rule_eval_result.append(py_exp_pred)\n",
    "        \n",
    "    rules['is_satisfy_instance'] = rule_eval_result\n",
    "    \n",
    "    g2_guide_df = rules[rules['is_satisfy_instance']==True]\n",
    "    g4_guide_df = rules[rules['is_satisfy_instance']==False]\n",
    "\n",
    "    g2_guide_df = g2_guide_df.sort_values(by='importance', ascending=False)\n",
    "    g4_guide_df = g4_guide_df.sort_values(by='importance', ascending=False)\n",
    "    \n",
    "    g2_guide = g2_guide_df.iloc[0]['rule']\n",
    "    g4_guide = g4_guide_df.iloc[0]['rule']\n",
    "    \n",
    "    return g2_guide, g4_guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rule(proj_name):\n",
    "    global_model, correctly_predict_df, indep, dep, feature_df = prepare_data_for_testing(proj_namez)\n",
    "    x_test, y_test = prepare_data(proj_name, mode = 'test')\n",
    "    \n",
    "    rq3_eval_result = pd.DataFrame() # for train data\n",
    "\n",
    "    py_exp_guide = []\n",
    "    lime_guide = []\n",
    "    \n",
    "    for i in range(0,len(feature_df)):\n",
    "        X_explain = feature_df.iloc[[i]]\n",
    "\n",
    "        row_index = str(X_explain.index[0])\n",
    "\n",
    "        py_exp = pickle.load(open(pyExp_dir+proj_name+'_rulefit_crossoverinterpolation_'+row_index+'.pkl','rb'))\n",
    "        lime_exp = pickle.load(open(pyExp_dir+proj_name+'_lime_'+row_index+'.pkl','rb'))\n",
    "\n",
    "        py_exp_local_model = py_exp['local_model']\n",
    "        lime_exp_local_model = lime_exp['local_model']\n",
    "        \n",
    "        py_exp_the_best_defective_rule_str = get_rule_str_of_rulefit(py_exp_local_model)\n",
    "#         print(py_exp_the_best_defective_rule_str)\n",
    "        \n",
    "        total_cond_in_py_exp = len(py_exp_the_best_defective_rule_str.split('&'))\n",
    "        \n",
    "        lime_the_best_defective_rule_str = lime_exp['rule'].as_list()[0][0]\n",
    "\n",
    "        py_exp_pred = eval_rule(py_exp_the_best_defective_rule_str, X_explain)[0]\n",
    "        lime_pred = eval_rule(lime_the_best_defective_rule_str, X_explain)[0]\n",
    "\n",
    "#         print(py_exp_pred, lime_pred)\n",
    "        if py_exp_pred:\n",
    "            py_exp_the_best_defective_rule_str = flip_rule(py_exp_the_best_defective_rule_str)\n",
    "            py_exp_guide.append(py_exp_the_best_defective_rule_str)\n",
    "\n",
    "        if lime_pred:\n",
    "            lime_the_best_defective_rule_str = flip_rule(lime_the_best_defective_rule_str)\n",
    "            lime_guide.append(lime_the_best_defective_rule_str)\n",
    "        \n",
    "    print(set(py_exp_guide))\n",
    "    print('total guidance:',len(set(py_exp_guide)))\n",
    "    print('-'*100)\n",
    "    print(set(lime_guide))\n",
    "    print('total guidance:',len(set(lime_guide)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_PyExplainer_guidance(proj_name, global_model, method_name, row_index, guidance, x_test, y_test_flip , flip=False):\n",
    "    guidance_list = guidance.split('&')\n",
    "    \n",
    "    guide_eval_result = pd.DataFrame()\n",
    "    \n",
    "    for condition in guidance_list:\n",
    "        if flip:\n",
    "            condition = flip_rule(condition)\n",
    "            \n",
    "        py_exp_guidance_eval = summarize_rule_eval_result(condition, x_test)\n",
    "\n",
    "        guide_prec = precision_score(y_test_flip, py_exp_guidance_eval)\n",
    "        guide_rec = recall_score(y_test_flip, py_exp_guidance_eval)\n",
    "\n",
    "        py_exp_serie_test = pd.Series(data=[proj_name, row_index, method_name, global_model,condition, guide_prec, guide_rec])\n",
    "        guide_eval_result = guide_eval_result.append(py_exp_serie_test,ignore_index=True)\n",
    "        \n",
    "    return guide_eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def rq3_eval(proj_name, global_model_name):\n",
    "    global_model, correctly_predict_df, indep, dep, feature_df = prepare_data_for_testing(proj_name, global_model_name)\n",
    "    x_test, y_test = prepare_data(proj_name, mode = 'test')\n",
    "    \n",
    "    y_test_flip = [False if val else True for val in y_test]\n",
    "    \n",
    "    rq3_explanation_result = pd.DataFrame()\n",
    "    \n",
    "    pyexp_guidance_result_list = []\n",
    "    lime_guidance_result_df = pd.DataFrame()\n",
    "    \n",
    "    for i in range(0,len(feature_df)):\n",
    "        X_explain = feature_df.iloc[[i]]\n",
    "\n",
    "        row_index = str(X_explain.index[0])\n",
    "\n",
    "        exp_obj = pickle.load(open(pyExp_dir+proj_name+'_'+global_model_name+'_all_explainer_'+row_index+'.pkl','rb'))\n",
    "        py_exp = exp_obj['pyExplainer']\n",
    "        lime_exp = exp_obj['LIME']\n",
    "\n",
    "        py_exp_local_model = py_exp['local_model']\n",
    "        lime_exp_local_model = lime_exp['local_model']\n",
    "        \n",
    "        py_exp_the_best_defective_rule_str = get_rule_str_of_rulefit(py_exp_local_model)\n",
    "        lime_the_best_defective_rule_str = lime_exp['rule'].as_list()[0][0]\n",
    "\n",
    "        py_exp_pred = eval_rule(py_exp_the_best_defective_rule_str, X_explain)[0]\n",
    "        lime_pred = eval_rule(lime_the_best_defective_rule_str, X_explain)[0]\n",
    "\n",
    "        if py_exp_pred:\n",
    "        \n",
    "            condition_list = py_exp_the_best_defective_rule_str.split('&')\n",
    "\n",
    "            # for explanation\n",
    "            for condition in condition_list:\n",
    "                condition = condition.strip()\n",
    "                \n",
    "                py_exp_rule_eval = summarize_rule_eval_result(condition, x_test)\n",
    "\n",
    "                rule_prec = precision_score(y_test, py_exp_rule_eval)\n",
    "                rule_rec = recall_score(y_test, py_exp_rule_eval)\n",
    "\n",
    "                py_exp_serie_test = pd.Series(data=[proj_name, row_index, 'pyExplainer',global_model_name, condition, rule_prec, rule_rec])\n",
    "                rq3_explanation_result = rq3_explanation_result.append(py_exp_serie_test,ignore_index=True)\n",
    "\n",
    "            # for guidance\n",
    "            g2_guide, g4_guide = get_g2_g4_guidance(py_exp_local_model, X_explain)\n",
    "            \n",
    "            flip_guide_eval_result = eval_PyExplainer_guidance(proj_name, global_model_name,'PyExpFlip', row_index,\n",
    "                                                               py_exp_the_best_defective_rule_str, x_test, y_test_flip, flip=True)\n",
    "            g2_guide_eval_result = eval_PyExplainer_guidance(proj_name, global_model_name,'PyExpG2', row_index,\n",
    "                                                               g2_guide, x_test, y_test_flip, flip=False)\n",
    "            g4_guide_eval_result = eval_PyExplainer_guidance(proj_name, global_model_name,'PyExpG4', row_index,\n",
    "                                                               g4_guide, x_test, y_test_flip, flip=False)\n",
    "        \n",
    "            pyexp_guidance_result_list.append(flip_guide_eval_result)\n",
    "            pyexp_guidance_result_list.append(g2_guide_eval_result)\n",
    "            pyexp_guidance_result_list.append(g4_guide_eval_result)\n",
    "            \n",
    "#             break\n",
    "        \n",
    "        if lime_pred:\n",
    "            lime_rule_eval = summarize_rule_eval_result(lime_the_best_defective_rule_str, x_test)\n",
    "            \n",
    "            rule_prec = precision_score(y_test, lime_rule_eval)\n",
    "            rule_rec = recall_score(y_test, lime_rule_eval)\n",
    "            \n",
    "            lime_serie_test = pd.Series(data=[proj_name, row_index, 'LIME',global_model_name, lime_the_best_defective_rule_str, rule_prec, rule_rec])\n",
    "            rq3_explanation_result = rq3_explanation_result.append(lime_serie_test,ignore_index=True)\n",
    "            \n",
    "            lime_guidance = flip_rule(lime_the_best_defective_rule_str)\n",
    "            lime_guidance_eval = summarize_rule_eval_result(lime_guidance, x_test)\n",
    "#             tn, fp, fn, tp = confusion_matrix(y_test, lime_rule_eval, labels=[1,0]).ravel()\n",
    "#             tp_rate = tp/(tp+fn)\n",
    "#             tn_rate = tn/(tn+fp)\n",
    "            \n",
    "            guide_prec = precision_score(y_test_flip, lime_guidance_eval)\n",
    "            guide_rec = recall_score(y_test_flip, lime_guidance_eval)\n",
    "            \n",
    "            lime_serie_test = pd.Series(data=[proj_name, row_index, 'LIME', global_model_name, lime_guidance, guide_prec, guide_rec])\n",
    "            lime_guidance_result_df = lime_guidance_result_df.append(lime_serie_test, ignore_index=True)\n",
    "            \n",
    "        print('finished {} from {} commits'.format(str(i+1),len(feature_df)))\n",
    "        \n",
    "        \n",
    "    pyexp_guidance_result_df = pd.concat(pyexp_guidance_result_list)\n",
    "    \n",
    "    rq3_guidance_result = pd.concat([pyexp_guidance_result_df, lime_guidance_result_df])\n",
    "    \n",
    "    \n",
    "    \n",
    "    rq3_explanation_result.columns = ['project','commit_id','method','global_model','explanation','precision','recall']\n",
    "    rq3_guidance_result.columns = ['project','commit_id','method', 'global_model','guidance','precision','recall']\n",
    "\n",
    "    rq3_explanation_result.to_csv(result_dir+'RQ3_'+proj_name+'_'+global_model_name+'_explanation_eval_split_rulefit_condition.csv',index=False)\n",
    "    rq3_guidance_result.to_csv(result_dir+'RQ3_'+proj_name+'_'+global_model_name+'_guidance_eval_split_rulefit_condition.csv',index=False)\n",
    "\n",
    "    \n",
    "    #         break\n",
    "        \n",
    "#     rq3_eval_result.columns = ['project', 'commit id', 'method', 'explanation','guidance', 'explantion_precision','explantion_recall', \n",
    "#                               'guidance_precision','guidance_recall', 'explanation_true_negative', 'explanation_false_positive',\n",
    "#                               'explanation_false_negative','explanation_true_positive']\n",
    "    \n",
    "#     rq3_eval_result.to_csv(result_dir+'RQ3_'+proj_name+'_'+global_model_name+'_split_rulefit_condition.csv',index=False)\n",
    "#     print('finished RQ3 of',proj_name)\n",
    "    \n",
    "#     display(rq3_eval_result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rq3_eval('openstack','RF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openstack\n",
      "finished 1 from 198 commits\n",
      "finished 2 from 198 commits\n",
      "finished 3 from 198 commits\n",
      "finished 4 from 198 commits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-8a52646757aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'openstack'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrq3_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'openstack'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'RF'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'finished in'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'secs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-fbedfc2b1887>\u001b[0m in \u001b[0;36mrq3_eval\u001b[0;34m(proj_name, global_model_name)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mlime_guidance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflip_rule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlime_the_best_defective_rule_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mlime_guidance_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarize_rule_eval_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlime_guidance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;31m#             tn, fp, fn, tp = confusion_matrix(y_test, lime_rule_eval, labels=[1,0]).ravel()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m#             tp_rate = tp/(tp+fn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-cecffb0f841c>\u001b[0m in \u001b[0;36msummarize_rule_eval_result\u001b[0;34m(rule_str, x_df)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msummarize_rule_eval_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrule_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;31m#     print('Rulefit')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mall_eval_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_rule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrule_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mall_eval_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_eval_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-cecffb0f841c>\u001b[0m in \u001b[0;36meval_rule\u001b[0;34m(rule, x_df)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# if the rule does not satisfy clean commit, the truth value of the inversed rule when applied to clean commit is true\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0meval_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvar_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0meval_result_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print('openstack')\n",
    "rq3_eval('openstack','RF')\n",
    "end = time.time()\n",
    "print('finished in',str(end-start), 'secs')\n",
    "\n",
    "start = time.time()\n",
    "print('qt')\n",
    "rq3_eval('qt','RF')\n",
    "end = time.time()\n",
    "print('finished in',str(end-start), 'secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "print('openstack')\n",
    "rq3_eval('openstack','LR')\n",
    "end = time.time()\n",
    "print('finished in',str(end-start), 'secs')\n",
    "\n",
    "start = time.time()\n",
    "print('qt')\n",
    "rq3_eval('qt','LR')\n",
    "end = time.time()\n",
    "print('finished in',str(end-start), 'secs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_Oat",
   "language": "python",
   "name": "env_oat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
